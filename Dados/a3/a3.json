{
  "Introduction": [
    "Introduction:\nMisinformation, in its broadest sense, encompasses “any information that is demonstrably\nfalse or otherwise misleading, regardless of its source or intention” (van der Linden et al., 2023,\np. 7). The defining characteristic of misinformation lies in its (lack of) truth value: Information\nthat is inaccurate, incomplete, misleading, or false information as well as selective and halftruths. However, the intention remains an important dimension. Consider a white lie,\ninformation that is untrue, maybe misleading, but with a benign intention. It is rather safe to\nassume that white lies are not viewed as misinformation. Amid ongoing debates on the most\nappropriate criterion against which the true value or ground truth of a piece of information\nshould be  assessed, several key benchmarks emerge, including evidence or facts, expert\nopinion, and characteristics of deceptive or manipulative techniques (Nan et  al., 2023;\nRoozenbeek et al., 2022; Vraga and Bode, 2020). While some distinguish misinformation and\ndisinformation, with the latter being purposefully created and shared to deceive and cause\nharm, we consider disinformation as a subset of misinformation, recognizing the inherent\nchallenge of detecting intent in many cases (Treen et al., 2020).",
    "The widespread dissemination of misinformation poses one of the greatest challenges\nfacing contemporary human society, which is exacerbated by the high-choice media and social\nmedia environment. While misinformation is not new and its history dates back to the early\n15th century (Soll, 2016), its increasingly evident and severe ramifications across a wide array\nof public domains, including science, health, and politics, among others, have gained the\nphenomenon unprecedented attention in recent decades, particularly post-2016, the year\nwhen “post-truth” was selected as the word of the year by the Oxford dictionary. Indeed,\nmisinformation has been documented to undermine electoral processes (Berlinski et al., 2023),\ndiminish support for proclimate policies (Treen et al., 2020), fuel vaccination hesitancy (Garett and Young, 2021), and discourage the enactment of preventive and\nsafety behaviors during the coronavirus diseases 2019 (COVID-19)\npandemic (Greene and Murphy, 2021). Beyond mere facts, a pressing\nconcern is understanding how and why misinformation influences\nindividuals’ decision-making and yields these detrimental outcomes. Over the years, there has been a shift from a paradigm heavily\nemphasizing information deficits, which attributes the impact of\nmisinformation to a lack of understanding and knowledge about facts,\nto a more nuanced framework recognizing that ignorance alone lacks\nexplanatory power (Ecker et al., 2022). This transition acknowledges\nthe mounting evidence that people persist in making decisions based\non misinformation despite retractions and corrective efforts (Seifert,\n2014; Thorson, 2016)—some parents’ belief in the measles, mumps,\nand rubella (MMR) vaccines–autism link persists long after the\ninfamous 1998 Lancet article was retracted (in 2010) and debunked\n(in 2011) is a good example.",
    "Misinformation can occur and spread without any prior position,\nparticularly when it is due to a lack of knowledge (e.g., even though it\nwas established in the third century by Aristarchus and Eratosthenes\nthat the earth was round, the idea was not widely accepted until\naround the 15th century; Uri, 2020). It is not a coincidence, however,\nthat misinformation is most rampant on issues that are controversial\nand politically charged (e.g., climate change, the 2020 election,\nCOVID-19, etc.), given the polarization and dividedness of society in\nthe post-2016 United States. This suggests that it is most likely that\nindividuals already have formed their opinions and/or developed\norientations toward the specific issues before misinformation is\ngenerated and spread. In this article, we take the perspective of biased\nand motivated information processing to understand and explain the\nimpact and persistence of misinformation. Specifically, we examine\nresponses to scientific evidence or corrective messages as a special case\nof the processing of counterattitudinal information driven by multiple\nmotivations and cognitive fallacies. Conversely, misinformation as\nattitude-consistent messages undergoes similarly biased processing. It\nshould be noted that biased and motivated processing of scientific\nevidence and facts by individuals with the correct positions do not\nhave any other undesirable consequences (e.g., Shen et al., 2009),\nalthough they equally contribute to the polarization and dividedness\nof society as those to whom misinformation is attitude-consistent. There is evidence that greater partisan divisions in social reality\n(Nyhan, 2021) and a stronger desire for shared reality (Jost et al., 2018)\nlead to increased susceptibility to misinformation and willingness to\nshare/spread misinformation. Hence, we have Proposition 1: Proposition 1: Misinformation is more prevalent, influential, and\npersistent on topics/issues that are controversial and politically\ncharged than neutral or non-divisive ones.",
    "Admittedly, misinformation often has attention-grabbing features\nthat make it attractive and easy to process. For example, compared to\nfactual news, misinformation tends to be more emotional and less\nlexically diverse and demonstrates greater readability (Carrasco-Farré,\n2022). It is often high in sensationalism (Staender et al., 2022) and\ncontains negative or threatening content, social cues, and the presence\nof celebrities (Acerbi, 2019). In addition, misinformation frequently\nemploys manipulative techniques. A systematic review identified the\nuse of logical fallacies and misrepresentations, cherry-picking, fake\nexperts, impossible expectations, and conspiracy theories as common message features shared by most misinformation (Schmid et al., 2023). Roozenbeek et  al. (2022) outlined five epistemologically dubious\nstrategies commonly observed in online misinformation, including\n(1) the use of emotionally charged language that evokes fear, anger,\nand other negative emotions, (2) the presentation of incoherent or\nmutually exclusive arguments, (3) the framing of issues in false\ndichotomies, (4) the scapegoating of individuals or groups to reduce\nthe complexity of a problem, and (5) the resort to ad hominem attacks\nthat target the speaker rather than their arguments. Undoubtedly,\nthese features of misinformation are conducive to the spread and\npersistence of misinformation (Kemp et al., 2022; Putman et al., 2017). Yet, cognitive theories suggest that it is primarily through individuals’\ncognitive processing that misinformation persists while correction\nfails. That is, these message features feed into the motivational and\ncognitive biases in information responses, especially when individuals\nalready hold a preexisting belief in misinformation, which then shapes\nthe outcomes of the messages. Recognizing the influence of these\nmessage features, we now turn our discussion to the motivational and\ncognitive biases that underpin the persistence of misinformation."
  ],
  "Development": [
    "Multiple motivational and cognitive mechanisms may contribute\nto how people engage with misinformation and corrective information\n(Desai et al., 2020; Kunda, 1990). We cluster these mechanisms into\nthree groups. The first one consists of more deliberate motivations. These motivations concern goals that individuals actively or\nconsciously pursue to achieve a desired state. The second category\nincludes more automatic motivations. It differs from the first one in\nthat these goals tend to be triggered automatically and the goal pursuit\nmight be more or less unconscious. The third class of mechanisms\ncenters on cognitive fallacies, which refer to a type of information\nprocessing that reduces accuracy and results in erroneous and/or\nirrational conclusions. Cognitive fallacies are distinct from the\nmotivations and can occur absent any motivations.",
    "Humans operate on three fundamental motivations: value,\ncontrol, and truth (Cornwell and Higgins, 2018). These motivations\nare essential to how people seek effectiveness (Higgins, 2014). Value\nmotivation is concerned with having desired results (i.e., expected\nutility), especially as they relate to avoiding pain and cost and gaining\npleasure and benefits. Control motivation involves making things\nhappen and managing what happens and how it happens. Truth\nmotivation pertains to the establishment of what is real vs. imaginary\nand what is right vs. wrong (Cornwell and Higgins, 2018). All three\nmotivations suggest that individuals are not intrinsically motivated by\naccuracy as the dual process models (Chaiken et al., 1989; Petty and\nCacioppo, 1986) suggest they might: Even the truth motivation\ninvolves subjective and value-laden judgments (i.e., what is right vs.\nwrong); although conceivably, the motivations of value and control\nmay play a particularly crucial role in the creation, spread, and\npersistence of misinformation. Individuals may generate, biasedly\nprocess, and disseminate misinformation and avoid interacting with others who disagree with them to secure desired outcomes and exert\ncontrol. This could be especially true among those who already hold\nmisinformed beliefs. By perpetuating misinformation and resisting\ncorrective efforts, they fulfill their goals of preserving a favorable\nstatus quo, avoiding unwanted threats, and potentially enhancing their\ninfluence and gaining dominance (Cornwell et  al., 2014). The\nsubjectivity in, or worse yet, the lack of, truth motivations further\nexplains why corrective information presenting facts and evidence\noften fails to persuade—accuracy and validity do not necessarily\nsatisfy the truth motivation (i.e., when they are evaluated along the\ndimension of right vs. wrong). Facts and evidence tend to be ignored,\nand their sources are rejected when they do not render the value (i.e.,\nexpected utility) or control (cf. Festinger, 1957).",
    "In line with Higgins and colleagues’ tripartite framework of\nmotivation, Sharot and Sunstein (2020) identified instrumental\n(action), hedonic (affect), and cognitive (cognition) utilities as three\nmotivations that guide people’s informational behavior. People are\nmotivated to acquire and accept information they perceive as\nfacilitating decision-making and actions that maximize rewards and\nminimize harm, inducing positive emotions evading negative ones,\nand enhancing their ability to understand and predict realities. Instrumental and hedonic utilities operate in similar ways as control\nand value motivations: The pursuit of action-facilitative knowledge\nand positive affect can drive individuals to adopt misinformation and\nresist corrective information. Indeed, people manage what information\nto seek, accept, and believe in as an emotion regulation strategy\n(Heinström et al., 2022; Vrinten et al., 2022). Cancer patients may\nbe motivated to endorse treatment-related misinformation due to the\nhope it gives. When driven by hope, people experience reduced\nmessage fatigue (Shen et al., 2022), displaying increased openness to\n(mis)information they consider useful. Considerations of cognitive\nutility—more specifically, the motivation to minimize the gap between\none’s mental representations and external reality and thus have a\nsecured sense of comprehension—may lead to avoidance of\ninformation that threatens existing mental models (Sharot and\nSunstein, 2020). Consequently, corrective information tends to\nbe  assigned negative cognitive values whereas misinformation is\nconsidered to have positive cognitive values to satisfy the need to align\ninternal cognitions with (distorted) external realities.",
    "Despite the differences, the more recent theorizations on\nmotivations are not incompatible with the dual process models. The\nheuristic and systematic processing model (HSM) propose people\nmay be  driven by accuracy, defense, and impression motivations\n(Chaiken et al., 1996), which are, respectively, aligned with outcome-,\nvalue-, and impression-relevant involvement (Johnson and Eagly,\n1989). Accuracy motivation is the desire to arrive at valid attitudes or\nbeliefs that correspond with reality. It is closely associated with\noutcome-relevant involvement where one’s attitudes are primarily\nconcerned with direct personal consequences and concrete utility. Accuracy motivation promotes in-depth and careful processing of\ninformation that allows one to reach accurate conclusions. Defense\nmotivation refers to the desire to hold attitudes or beliefs that are\nconsistent with self-definition. Associated with value-relevant\ninvolvement where a person’s attitude is primarily linked to one’s selfidentity and values, defense motivation drives people to process\ninformation in ways that preserve their self-definition and -concept. Impression motivation is the desire to express attitudes and beliefs that\nsatisfy interpersonal and social goals. It corresponds with impression-relevant involvement that highlights the selfpresentational and social–relational consequences of one’s expressed\nattitudes. When impression-motivated, people engage in processing\nstrategies that yield conclusions that enable social acceptance. In the\ncase of misinformation, motivations other than accuracy presumably\nunderlie individuals’ processing. As individuals form a preexisting\nbelief in misinformation, particularly on highly controversial or\npolarized issues, their position becomes closely tied to their personal\nvalues, identities, and social belongingness, which—when confronted\nwith correction—triggers defense and/or impression motivations and\nthereby leads to biased processing that allows them to arrive at\nconclusions that favor their existing misconceptions (Jost et al., 2022;\nTrevors, 2019). Accuracy motivation, an assumption central to\ncorrective messages presenting factual evidence, on the contrary, is\nabsent or overshadowed, limiting the utility of corrective efforts. Hence, we have Proposition 2:\nProposition 2: The impact and persistence of misinformation are\npositively associated with non-accuracy motivations.",
    "Cognitive consistency theories, such as balance theory (Heider,\n1946), congruity theory (Osgood and Tannenbaum, 1955), and\ncognitive dissonance theory (Festinger, 1957), share a common\nassumption that people are driven to maintain consistency among\nelements of their cognitions (i.e., units of information). The goal of\nrestoring consistency tends to be  evoked by the psychological\ndiscomfort resulting from inconsistency and may occur beneath\nconscious awareness without deliberate intent. Balance theory, for\nexample, postulates that balance—a psychologically pleasant, desirable\nstate—is achieved when all cognitive elements are internally\nconsistent. More specifically, often studied within the P-O-X\nframework where P refers to the person/self, O refers to the other\nperson, and X refers to the object, balance is described as the state in\nwhich a person (dis)agrees with another person they (dis)like on the\nattitudinal object (Heider, 1946). When in an imbalanced state, people\nare motivated to change one or more relationships in the triad. Congruity theory shares similar propositions as balance theory.\nHowever, it is more precise than balance theory in that it quantifies\nthe degree of liking of the other person and one’s attitude toward the\nobject (Osgood and Tannenbaum, 1955). As such, congruity theory\nallows a more accurate prediction.",
    "Cognitive dissonance theory differentiates three types of\nrelationships between cognitive elements: irrelevant, consonant, and\ndissonant (Festinger, 1957). As the ratio of dissonant to consonant\ncognitions and the importance of dissonant elements increase,\nindividuals experience greater cognitive dissonance and greater\nmotivation to reduce it. Correspondingly, the reduction can\nbe  achieved in ways such as adding new consonant cognitions,\nremoving dissonant cognitions, and changing the importance of\nconsonant and/or dissonant cognitions. These cognitive consistency\ntheories suggest that when there is a discrepancy between the\nadvocacies of the message and individuals’ preexisting positions,\npeople are automatically motivated to close that gap, which likely leads\nto message resistance and rejection. This means that when a belief in\nmisinformation is established, any corrective information threatens the preferred state of consistency and arouses motivations to dismiss,\ndistort, or deny it. People may also seek and internalize information\nthat reinforces their existing misconceptions. Within the framework\nof cognitive dissonance, individuals who already believe in\nmisinformation might still be open to facts and evidence, that is, when\nthere are external justifications that reduce cognitive dissonance. However, even when that happens, the impact of facts and evidence is\nmost likely in the form of compliance instead of identification or\ninternalization (Kelman, 1958). Attitude change in the form of\ncompliance tends to be flimsy and short in duration, which explains\nwhy fact-checking might have a short-term effect in mitigating\nmisinformation but is positively associated with the persistence of\nmisinformation (Chan et al., 2017).",
    "A closely related theory to cognitive consistency theories is the\nself-affirmation theory (Steele, 1988). Its distinguishing feature lies in\nits premise that people are primarily driven to maintain self-integrity\n(rather than cognitive consistency). Self-integrity is the perception of\nthe self “as adaptively and morally adequate, that is, as competent,\ngood, coherent, unitary, stable, capable of free choice, capable of\ncontrolling important outcomes, and so on” (Steele, 1988, p. 262). When encountering information that threatens one’s perceived selfintegrity, people are motivated to restore it. Although accepting the\ninformation and changing one’s attitude is an option, it is often\nunlikely due to the threat it poses to the fundamental aspects of one’s\nidentity. Rather, people are more inclined to engage in defensive\nresponses, which can be automatic in nature, to avoid or dismiss the\nthreat (Sherman and Cohen, 2006). This process clearly sheds light on\nwhy beliefs in misinformation tend to be  resistant to correction. However, it is worth noting that divergent from cognitive consistency\ntheories, self-affirmation theory proposes a third alternative\nadaptation mechanism, which is to self-affirm a different domain of\nidentity that is not necessarily related to the threat (Steele, 1988). Putting this differently, when self-integrity is bolstered through selfaffirming some alternative aspects of the self, people may find\ncognitive inconsistency tolerable and respond more openly and\nobjectively to threatening information (Sherman and Cohen, 2006;\nSteele and Spencer, 1992). Nevertheless, this seemingly promising\ntenet may encounter challenges in practice given the polarization of\nsociety across various interconnected issues and the intricate\nintertwining of important domains of individuals’ identities, which\noften align with one another. This can undermine the applicability of\nthis adaptation, giving way to defensive responses.",
    "Social judgment theory (Sherif et al., 1965) is a third relevant\nframework in this cluster. The theory posits that attitude change is a\nfunction of one’s judgment of the position advocated by a message in\nrelation to their existing position. Specifically, depending on where an\nindividual’s initial position is located on a continuum of all potential\nalternative positions, one might evaluate a message’s advocacy as\nwithin their latitude of acceptance (i.e., positions that one considers\nacceptable), rejection (i.e., positions that one considers unacceptable),\nor non-commitment (i.e., positions that one considers neither\nacceptable nor unacceptable). Attitude change is likely to be greatest\nwhen a message’s advocacy is most distant from one’s existing position\nbut does not cross into the latitude of rejection. When a position falls\nwithin the latitude rejection, conversely, persuasion is unlikely and\nmay even cause backfire. Corrective messages as counterattitudinal\ninformation reside in the latitude of rejection, eliciting automatic\nresistance against the message and subsequently the observed ineffectiveness. Moreover, social judgment theory suggests that the\nsize of the latitude varies based on the level of ego involvement. As the\nissue in question is of greater personal importance to an individual\n(i.e., greater ego involvement; Sherif et al., 1973), which is frequently\nthe case given the highly politicized and divided nature of topics on\nwhich misinformation proliferates, the latitude of rejection expands,\nheightening the likelihood of contrast effect wherein an advocated\nposition is perceived more distant from their position than it actually\nis (Sherif et  al., 1965; Sherif and Hovland, 1961). Consequently,\nexisting beliefs in misinformation fail to be  corrected and might\nbe further reinforced.",
    "There has been evidence that conservatives are more likely than\nliberals to prioritize values of conformity and tradition, possess a stronger\ndesire to share reality with like-minded others, and perceive withingroup consensus when making judgments (Jost et al., 2018). Along with\nthe features of misinformation such as false dichotomy, scapegoating,\nand hominem attacks (Roozenbeek et al., 2022), these motivational\ntendencies of the conservatives make them more susceptible to\nmisinformation than liberals (Garrett and Bond, 2021), although both\nconservatives and liberals are gullible (e.g., van der Linden et al., 2020). Conservatives are also more likely than liberals to be influenced by\nrelational cues and sources who are similar to themselves, maintain\nhomogenous social networks, and favor echo chamber environments\n(Jost et al., 2018). Hence, we have Propositions 3 and 4:\nProposition 3: Consistency- and self-identity-related motivations\nare positively associated with misinformation effects\nand persistence. Proposition 4: The impact of misinformation is more pronounced\nand its duration more persistent among conservatives\nthan liberals.",
    "Biased responses may occur due to cognitive fallacies in the\nabsence of motivations (MacCoun, 1998). A wide range of cognitive\nfallacies might be  relevant to understanding the persistence of\nmisinformation within the framework of the processing of\ncounterattitudinal information. Here, we  focus on a few most\npervasive ones that are closely related to (selective) exposure to\ninformation and the subsequent biases in decision-making.",
    "Epistemic egocentrism is a form of perspective-taking failure\nwhere individuals are unable to set aside their privileged information\nthat they know is unavailable to others, leading to predictions that\nskew others’ perspectives toward one’s own (Royzman et al., 2003). This tendency means that people have a hard time adopting viewpoints\nthat are not their own and struggle to process and understand\ncounterattitudinal information, such as corrective information (Shen\nand Zhou, 2021). The failure to take others’ perspectives may closely\ninterplay with the conviction that one’s own perspectives objectively\nreflect reality and that those who do not share the same perspectives\nare uninformed and incompetent, a bias termed objectivity illusion\n(Schwalbe et al., 2020). The illusion that one is immune to bias (see\nalso biased blind spot, Pronin et al., 2002) exacerbates the difficulty of\nrectifying misconceptions as attitude-incongruent messages are\ndismissed as distorted and irrational.",
    "Choice blindness is a fallacy closely related to epistemic\negocentrism. Choice blindness refers to the inability to detect a\ndiscrepancy between one’s intended choice and the choice presented\nto them (Johansson et al., 2005, 2006). It is the tendency to be unaware\nthat one’s choices and preferences have been changed or manipulated\nafter the decision is already made. Epistemic egocentrism leads people\nto (1) look inward to examine their own thoughts and inner states\nsuch as emotions, preexisting judgments, and perceptions, (2)\n(incorrectly) believe they fully understand the roots of their inner\nstates, but (3) assume that other people’s introspections are largely\nunreliable. When people have formed their positions and opinions\nbased on misinformation, more or less due to manipulations, choice\nblindness means they tend to rationalize and justify this manipulated\ndecision and adjust their attitudes to align with the misinformation\n(Stille et  al., 2017), while deeming (counterattitudinal) facts and\nevidence as from other people’s inner states, hence largely unreliable.",
    "Confirmation bias, is the tendency to seek, interpret, and remember\nevidence in ways that favor existing beliefs or expectations (Nickerson,\n1998; Oswald and Grosjean, 2004), can be considered a special case of\nepistemic egocentrism. It can occur without the presence of motivations\nand, in some cases, involuntarily, such that people may fall for this bias\neven when they have no obvious personal interest (Gilead et al., 2019;\nNickerson, 1998), leading misinformation-believing individuals to\nreject correction regardless of their involvement (Zhou and Shen, 2022). One consequence of confirmation bias is selective exposure, a tendency\nfor individuals to preferentially seek, attend to, and engage with\ninformation that is consistent with their inner states (i.e., preexisting\nbeliefs, values, and attitudes), positive in valence, high in sensational\nvalue, and easy to process. Often co-occurring with confirmation bias\nand selective exposure is another cognitive fallacy, illusory correlation. Illusory correlation describes the fallacy of perceiving a correlation\nwhere none exists or perceiving a stronger correlation than it really is\n(Hamilton and Rose, 1980). The (mis)belief in the MMR vaccine–\nautism link and the (mis)belief that the influenza vaccine gives one the\nflu are good examples of illusory correlation. It should be noted that\nillusory correlation happens not only to lay individuals but also to welltrained social scientists—type I error is not uncommon. Publication\nbias and self-confirmation bias often drive some scientists to cling to\ntheir theories in the face of disconfirming data that call for the rejection\nof their own theories (Kuhn, 1962). Underlying this bias is the process\nwhere preexisting attitudes or beliefs prime individuals to search for\nsupportive evidence, even when it is lacking, which allows one to\nmaintain their currently held position.",
    "Research on the continued influence effect of misinformation\noffers insights into how mental models and memory processes\ncontribute to the persistence of misinformation. People construct\ncausal chains of events (van den Broek, 1990), in which misinformation\nmay play a causal role (Johnson and Seifert, 1994). Correction,\nespecially when it does not provide a causal alternative, can disrupt\nthe causal structure supported by misinformation and leave people\nwith an incomplete model, such that people continue to resort to\nmisinformation for comprehension (Johnson and Seifert, 1994). Theories regarding information retrieval offer alternative,\ncomplementary explanations. People are susceptible to various\nreactivation and retrieval failures, such as misattributing the source of\nthe misinformation (vs. corrective information), insufficiently linking\na correction to the misinformation in memory such that\nmisinformation is retrieved unchecked despite the correction, and selectively retrieving misinformation in an automatic manner where\nthe false tag attached to it by correction is not co-activated (for a\nreview, see Ecker et al., 2022; Lewandowsky et al., 2012). Furthermore,\ninsofar as misconceptions are formed in memory and that they are\noften inevitably repeated in a correction, misinformation becomes\neasier to process with greater familiarity, fluency, and accessibility,\nwhich may serve as cues that imply its truth and hedonic value and\nlead to more positive evaluations (Alter and Oppenheimer, 2009;\nSwire et al., 2017; Winkielman et al., 2003). Indeed, while corrections\nmay suppress misinformation, they do not replace it (Gordon et al.,\n2017; Shtulman and Valcarcel, 2012); rather, they coexist and compete\nsuch that corrections have to override and inhibit misinformation for\nbeliefs to be updated (Potvin et al., 2015; Trevors, 2019). This process,\nhowever, requires effortful, deliberate thinking, which might\nbe hindered by cognitive miserliness or the tendency to default to less\ncostly processing mechanisms (Stanovich, 2021; Pennycook and Rand,\n2019). Hence, we have Proposition 5:\nProposition 5: The impact and persistence of misinformation are\ndriven by various cognitive fallacies, with or without the\nmotivational forces.",
    "Collectively, the motivation and cognitive mechanisms reviewed\nin the previous section can give rise to various forms of resistance to\ncorrective efforts, which ultimately contribute to the polarization and\nthe persistence of misinformation effects. First, individuals might\nengage in selective exposure and attention where they selectively\naccess and attend to (mis)information that is consistent with their\nprior belief and avoid information that contradicts it (Guess et al.,\n2018; Hart et al., 2009; Knobloch-Westerwick and Meng, 2009), a\nstrategy that is further facilitated by social media environments\n(Franziska et al., 2019; Spohr, 2017). This means that corrections have\na difficult time reaching their target audience in a naturalist setting\nand, even when it does, individuals tend to place limited cognitive\nresources on it and are less likely to retain it.",
    "Resistance may also manifest as biased assimilation and weighting. Biased assimilation is the tendency to perceive attitude-congruent\ninformation as more valid than attitude-incongruent information\n(Ahluwalia, 2000; Lord et al., 1979). When corrective information is\ndifficult to refute, individuals resort to relative weighting where they\nassign less weight to attitude-inconsistent attributes and attach more\nweight to attitude-consistent ones in their decision-making\n(Ahluwalia, 2000). They may also reduce the impact of attitudeinconsistent information on their overall judgment of or attitude\ntoward an issue (Ahluwalia, 2000). Put simply, individuals may utilize\nthe information in a distorted way that allows them to sustain their\nexisting beliefs.",
    "Biased perception represents another group of strategies that are\nfrequently employed in response to corrections. It is concerned with\ncognitive responses to different aspects of a persuasive message, such\nas its content/arguments, source, and intent (Shen et al., 2009). This\ncan be reflected as, for example, source derogation, which involves\nquestioning the credibility and/or expertise of the source of\ncounterattitudinal information (Cameron et al., 2002; Zhou and Shen, 2022). Similarly, people may develop counterarguments against and\nrefute the content of the attitude-inconsistent message (Taber and\nLodge, 2006) as well as challenge the strategy used in the message\n(Fransen et al., 2015). They may also perceive a stronger manipulative\nintent from corrective messages (Shen et al., 2009), which is known to\ntrigger reactance (Brehm, 1966), a motivational state marked by anger\nand critical cognitions that leads to persuasion failure (Dillard and\nShen, 2005).",
    "The ultimate manifestation of resistance lies in the boomerang or\nbackfire effect (Hart and Nisbet, 2012; Nyhan and Reifler, 2010),\nwhich occurs when a corrective message produces effects opposite to\nwhat is intended, resulting in individuals developing an even stronger\nbelief in misinformation. This effect, alongside selective exposure and\nattention, biased assimilation and weighting, and biased perception,\nin turn, catalyzes polarization not only on an individual scale where\nbelief in misinformation becomes more entrenched and extreme but\nalso at the societal level such that its dividedness is further exacerbated. Together, these processes illuminate why corrective efforts fail and\nwhy the effects of misinformation persist.",
    "As we  have reviewed above, the intricate interplay of various\nmotivational and cognitive mechanisms driving individuals who have\na prior belief in misinformation prompts the adoption of diverse\nstrategies to resist correction and preserve their existing view. As such,\nunsurprisingly, correction strategies such as fact-checking often prove\nineffective in countering these misconceptions (Ecker et al., 2014;\nSeifert, 2014; Thorson, 2016). Meta-analytical evidence documented\nlarge effects for the persistence of misinformation despite debunking\nand observed that a more detailed debunking message was associated\nwith a stronger misinformation-persistence effect (Chan et al., 2017). The challenge is particularly apparent when it comes to real-world\nmisinformation, with research suggesting that the effectiveness of\ndebunking real-world misinformation may diminish by 60% in\ncomparison to constructed misinformation due to Walter and Murphy\n(2018). Indeed, a recent meta-analysis of correction effects in sciencerelevant misinformation found a non-significant effect, indicating that\nefforts to debunk misinformation on issues such as COVID-19 and\nclimate change were overall not successful (Chan and Albarracín,\n2023). While corrective messages may be more successful in certain\ninstances, it is evident that they do not eliminate the effect of\nmisinformation (Walter and Tukachinsky, 2020). With that in mind,\nin this section, we focus our discussion on inoculation as a promising\nprebunking strategy for mitigating misinformation effects.",
    "The concept of inoculation in persuasion draws from a medical\nanalogy: just as human bodies can be immunized against viruses,\nour attitudes and beliefs can also be shielded from persuasive attacks\n(McGuire, 1964). With vaccination, individuals receive a weakened\nform of the virus, which stimulates the production of antibodies and\nstrengthens the immune system without causing illness itself to\nsafeguard against potential threats from the virus. Similarly, in the\npersuasion context, a mild version of a counterattitudinal message that can activate defense mechanisms but is not too strong to\npersuade can confer resistance to counterinfluence.",
    "There are two main parts to an inoculation message: a forewarning\nmessage indicating an impending attack on one’s current views and a\nrefutational preemption treatment that provides content that one may\nemploy to refute challenges to their views (Pfau et al., 1997; Compton,\n2013). To elaborate, a forewarning message is closely relevant to a\nperceived threat as one of the key mechanisms in inoculation theory. A prerequisite for inoculation to succeed is for people to develop\nthreat perceptions as they lead to the realization of the vulnerability of\ntheir beliefs, which in turn motivates the building of defense systems\n(Compton and Pfau, 2005; Petty and Cacioppo, 1986). Although the\nmere presence of counterattitudinal messages can generate threat\nperceptions (i.e., intrinsic threat, McGuire, 1964), the presence of a\nforewarning message tends to be  more effective (McGuire and\nPapageorgis, 1962). Refutational preemption, on the contrary, involves\na two-sided message that presents a weakened form of arguments\nfrom an anticipated attack message along with counterarguments\nagainst the arguments. In addition to this more passive refutation\napproach, individuals are sometimes instructed to develop their own\nrefutation material (Compton and Ivanov, 2013; McGuire and\nPapageorgis, 1961). Regardless of the approach, refutational\npreemption serves two main functions: to provide content to be used\nto refute potential attacks and to allow the practice of counterarguing\n(Compton and Pfau, 2005; Wyer, 1974). It should be  noted that\ncounterarguing, a cognitive process that occurs postinoculation\ntreatment, is not the same and goes beyond refutational materials\npresented in a refutational preemption treatment (Compton, 2013). It\nis this cognitive process—motivated by the perceived threat from\nimpending attacks—that confers resistance (Insko, 1967).",
    "Notably, while McGuire and colleagues initially limited the beliefs\nthat could be protected by inoculation to cultural truisms (i.e., widely\nshared beliefs such as the benefits of tooth brushing where an attack\nseems impossible, McGuire, 1964), the application of inoculation is\nno longer confined to non-controversial topics. Indeed, the theory has\nbeen shown to successfully confer resistance to attitudes in a wide\nrange of controversial domains such as genetically modified food and\nvaccines (Banas and Rains, 2010; Compton and Pfau, 2005). In\naddition, much has been done to address whether the efficacy of\ninoculation is observed when the attack message presents the same\narguments that are countered in the refutational preemption treatment\n(i.e., “refutational-same”) extends to situations where the attack\nmessage raises novel arguments that differ from those addressed in the\ntreatment (i.e., refutational-different). Evidence from decades of\nresearch shows that “refutational-same and -different” treatments are\nequally effective (Banas and Rains, 2010; McGuire, 1962), suggesting\nthat inoculation offers an umbrella or blanket of protection. Further,\ninoculation may even offer cross-protection such that its protection\nspillover from one topic to other related topics (e.g., from condom use\nto binge drinking, Parker et al., 2012, 2016).",
    "Given that inoculation serves as a preemptive measure to counter\nmisinformation before its adoption, it likely provides a more effective\nsolution than corrective measures applied after exposure. Research applying inoculation to mitigate misinformation has generated\nimportant insights, supporting its utility. The majority of the early\nstudy was done in the context of climate change. For example, van der\nLinden et al. (2017a), testing the effectiveness of inoculation against\nthe prevailing misinformation that there is no consensus on humancaused climate change, found that the positive impact of a message\nemphasizing the scientific consensus on the issue was largely preserved\nby inoculation. Importantly, this effect held for both Democrats and\nRepublicans. In addition, research shows that inoculation produces full\nprotection against climate change misinformation with a one-week\ndelay between the treatment and the attack, demonstrating the\nlongevity of the inoculation effects (Maertens et al., 2020). Beyond\nclimate change, inoculation has been shown to be  effective in\nconferring resistance to misinformation in the realms of health (e.g.,\nJiang et al., 2022; Geegan et al., 2023), politics (e.g., Zerback et al.,\n2021), and marketing (e.g., Boman and Schneider, 2021), to name a few.",
    "Building on the umbrella protection effect, scholars have further\nextended this research by developing a “broad-spectrum immunity”\napproach to inoculation that is not specific to the claims or the topics\nof misinformation (Lewandowsky and Van Der Linden, 2021). Cook\net al.’s (2017) study represents one of the earliest studies that took this\napproach. In their study, they developed logic-based refutations that\nput into question the misleading techniques underlying\nmisinformation. The findings of their study suggested that this\nstrategy was effective in neutralizing the effect of misinformation\n(Cook et al., 2017). Similarly, Roozenbeek and van der Linden (2019a)\nfound that playing a game that involves creating news articles using\nvarious misleading tactics successfully inoculated individuals against\nfake news. Testing of specific techniques that are commonly used in\nmisinformation, such as impersonation, use of emotional language,\npolarization, conspiracy theories, trolling, discrediting, ad hominem\nattacks, incoherent arguments, scapegoating, and false dichotomies,\nshowed that inoculation against these techniques, either with an active\nor passive approach, improved people’s ability to resist misinformation\n(Roozenbeek et al., 2022; Roozenbeek and van der Linden, 2019b). In\nsum, there is strong evidence that inoculation that targets\nmanipulation tactics employed in the production of misinformation\ncan effectively generate broad-spectrum immunity to misinformation\n(see also Basol et al., 2020, 2021; Maertens et al., 2021; Roozenbeek\net al., 2020, 2021; Roozenbeek and van der Linden, 2020).",
    "Another important extension of inoculation scholarship focuses\non its potential to spread protection to build a societal-level immunity\nagainst misinformation, analogous to herd immunity in a medical\ncontext. When a large population is inoculated and becomes immune\nto misinformation, we  may effectively limit and control its spread\n(Lewandowsky and Van Der Linden, 2021; van der Linden et  al.,\n2017b). There is growing evidence that inoculation not only builds\nresistance but also increases willingness to talk about contested issues\n(Lin and Pfau, 2007; Lin, 2022). Postinoculation interpersonal\nconversations, in turn, reinforce the treatment effects and spread the\ntreatment to a larger audience (Ivanov et al., 2012). Indeed, Ivanov et al.\n(2015) observed that inoculation increased advocacy-driven talk, in\nwhich individuals share both treatment-specified and novel arguments,\nshedding light on its diffusing potential. Much research on the effect of\ncampaign-induced interpersonal communication suggests that the\ntreatment diffused through postinoculation talk can then successfully\nbuild resistance among its recipients (Dillard et al., 2022; Jeong and\nBae, 2018; Southwell and Yzer, 2007). In sum, inoculation may promote interpersonal talk that spreads the treatment, which in turn may\nprotect the conversation partners from misinformation and ultimately,\nthrough this process, help build immunity at a larger scale. It is no\nsurprise that the psychological inoculation strategy has witnessed\nincreased applications (Serhan, 2024).",
    "Hence, we have Proposition 6:\nProposition 6: Psychological inoculation is an effective strategy\nto mitigate the impact and persistence of misinformation."
  ],
  "Conclusion": [
    "In this article, framing responses to corrective information as a\nparticular case of processing counterattitudinal information, we offer\na motivational and cognitive account of the persistence of\nmisinformation. Individuals are often motivated, sometimes\nautomatically so, to preserve their existing attitudes. Even in the\nabsence of motivations, the process of changing and updating beliefs\nhas proven to be challenging due to various cognitive fallacies and\nmental mechanisms. Together, these motivational and cognitive\nfactors give rise to multiple forms of resistance strategies, which in\nturn contribute to the entrenchment of misinformation effects.",
    "Understanding these underlying processes helps elucidate why\ndebunking efforts frequently fail to be effective. A preemptive strategy,\nin contrast, has shown promise. By reviewing psychological\ninoculation theory and its application in prebunking misinformation,\nwe present evidence demonstrating that inoculation against either the\nclaims or manipulative techniques used in misinformation can confer\nresistance, thereby protecting individuals from misinformation. Further, through interpersonal processes, such inoculation efforts have\nthe potential the spread and establish protection at the societal level."
  ]
}