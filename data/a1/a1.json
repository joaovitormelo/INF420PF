{
  "Introduction": [
    "The COVID-19 pandemic not only posed a significant challenge to global healthcare systems but also gave rise to an unprecedented surge in misinformation, termed an “infodemic.” (Clemente-Suárez et al., 2022). This phenomenon, characterized by an overabundance of information—both accurate and false—created confusion, fear, and mistrust among the public (Infodemic, 2020). This study explores the factors driving COVID-19 misinformation, its spread via social media, its impact on public health, the effectiveness of mitigation efforts, and strategies to enhance resilience against misinformation in future health crises. Misinformation spreads through a structured process with multiple stages. Each stage influences how false information gains attention and persists.The spread begins with a source. This could be  an individual, a social media post, or a coordinated disinformation campaign. The source may include manipulated content, misinterpretations, or deliberate fabrications. These false narratives are often designed to shape public opinion. Once misinformation is introduced, amplification occurs. Social media algorithms and human behavior contribute to its rapid spread. Emotionally charged misinformation spreads faster than factual content. This happens because social media platforms prioritize engagement and virality over accuracy. As misinformation spreads, it enters echo chambers. In these closed networks, people mainly interact with like-minded individuals. This limits their exposure to corrective information. As a result, misinformation becomes more persistent and harder to correct.To counter misinformation, intervention points are necessary. Real-time fact‑checking, algorithm adjustments, and trusted messengers can help correct falsehoods. Healthcare professionals and community leaders play an important role in spreading accurate information. Digital literacy programs also help individuals assess the credibility of online content.The impact of misinformation depends on the effectiveness of interventions. If unchecked, false narratives continue to spread. However, timely corrections can increase public awareness and reduce the spread of misinformation. Understanding these processes helps in developing better strategies for preventing misinformation (Figure  1).Misinformation prevention involves multiple layers of intervention. The first layer focuses on reactive measures. Fact‑checking and content moderation help counter misinformation after it has spread. Organizations verify claims, and AI systems flag or remove misleading content. However, these methods often face delays. People who already believe misinformation may also resist corrections. The second layer includes proactive strategies. These aim to stop misinformation before it spreads widely. Digital literacy programs teach people to evaluate information critically. Trusted messengers, like healthcare experts, help spread accurate information. The success of these strategies depends on cultural factors and people’s pre‑existing beliefs. The third layer addresses structural solutions. This includes government regulations and AI‑driven moderation. Some governments have introduced laws to hold social media platforms accountable. AI systems are also used to detect misinformation early. However, these measures must balance misinformation control with concerns about censorship and algorithmic biases. Regulations also vary across different countries. The final goal is to reduce misinformation and strengthen public resilience. Combining reactive, proactive, and structural interventions creates a more effective and sustainable response. Continuous research, policy improvements, and collaboration across different sectors are necessary. This will help address the evolving challenges of misinformation (Figure 2). The spread of misinformation during the pandemic created many challenges for public health. It made existing problems worse and reduced people’s willingness to follow preventive measures. As a result, governments and healthcare organizations struggle to provide accurate and timely information. This added pressure made it harder to manage the crisis effectively (Kisa and Kisa, 2024). The novelty of this study is that it takes an interdisciplinary approach by combining psychology, technology,and policy. It goes beyond public health‑focused research by using a structured framework to analyze misinformation. Unlike previous studies, it highlights algorithmic transparency and content amplification. It also explores advanced solutions like AI‑based detection and blockchain verification. These insights offer new ways to manage misinformation effectively. Addressing this infodemic became a crucial priority to mitigate its impact on the pandemic response and recovery (Ferreira Caceres et al., 2022; Bhattacharya et al., 2021; Rodrigues et al., 2024). Historically, misinformation has been a recurring challenge during health crises (Kisa and Kisa, 2024; Rodrigues et  al., 2024). For instance, during the 1918 influenza pandemic, unfounded claims about cures and conspiracy theories about the origins of the virus proliferated through newspapers and word of mouth (Barry, 2004). Similarly, the Ebola outbreaks in West Africa saw widespread myths about the disease, leading to harmful practices like avoiding healthcare facilities (WHO, 2020b; Muzembo et al., 2022; Buseh et al., 2015). However, the COVID-19 infodemic was unique in its scale and intensity, driven by the global reach of digital platforms and the unprecedented speed of information dissemination (Pulido et al., 2020; WHO, 2020a). Understanding this phenomenon is critical for addressing future infodemics in an increasingly interconnected world."
  ],
  "Development": [
    "The aim of this systematic review is to synthesize the existing body of literature on the infodemic during the COVID-19 pandemic, with a focus on identifying its causes, manifestations, and implications, as well as the strategies employed to combat it.",
    "The review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 and the Quality of Reporting of Meta-analyses statement (Page et al., 2021). We explored the following research questions: 1. What were the key psychological, technological, and societal factors contributing to the spread of misinformation during the COVID-19 pandemic? 2. How did social media algorithms influence the amplification and dissemination of COVID-19 misinformation? 3. What were the public health consequences of misinformation on vaccine hesitancy and adherence to preventive measures? 4. How effective were fact‑checking initiatives, regulatory measures, and public education campaigns in mitigating misinformation? 5. What strategies can improve public resilience against misinformation in future health crises?",
    "The search terms were oriented according to the Population, Intervention, Comparison and Results (PICOS) approach, as shown in Table 1. Studies published between December 2019 and the present were included to ensure that the research focused on misinformation during the COVID-19 pandemic. Language: Only studies published in English were considered due to accessibility and consistency in analysis. Peer-reviewed journal articles, conference papers, and reputable preprints were included to ensure academic rigour. Studies specifically examining misinformation related to COVID-19, including its sources, spread mechanisms, psychological and social impacts, and mitigation strategies, were included. Studies published before December 2019 was excluded as they do not pertain to COVID-19 misinformation. Non-English studies were excluded due to language barriers and potential translation inconsistencies. Opinion pieces, editorials, blog posts, and non-peer-reviewed sources were excluded to maintain academic reliability. Studies that addressed misinformation in general but did not specifically focus on COVID-19 were excluded from the review.",
    "We designed the search strategy with an information specialist using medical subject headings and specific keywords (Table 2). We included articles published in English from December 2019 to December 2024, focusing on the infodemic during the COVID-19 pandemic. Non-English papers were excluded due to language bias, resource constraints for translation, limited accessibility, and inconsistent quality control across languages, which may affect the reproducibility and comparability of findings. We searched four databases (PubMed, Scopus, Web of Science, and Google Scholar) and explored the included studies reference lists. Potential limitations of including only these four databases can include database, exclusion of grey literature, publication bias and indexing limitations. Boolean operators like AND is used to narrow a search by including all specified terms, ensuring that results contain each keyword and OR is used to broaden a search by retrieving results that contain at least one of the specified terms. We first conducted the search on 4 December 2024, and we re‑ran the search on 6 Jan 2024. After removing duplicates, two authors independently screened the title, abstract and full text of articles and included eligible articles for evaluation. An independent third author resolved any disagreements.",
    "Two independent researchers extracted the general characteristics of each study and classified them into seven major themes: 1. The Role of Digital Platforms in Amplifying Misinformation, 2. Interventions to Combat the Infodemic. 3. The Role of Trusted Messengers 4. Proactive Regulation of Digital Platforms, 5.Enhancing Health and Media Literacy, 6.Bridging the Digital Divide, 7.Technological Innovations in Misinformation Management. We clustered articles based on similar properties associated with the stated objective and the reported outcomes. Although infodemics were primarily defined as the overabundance of information, usually with a negative connotation, we decided to report data from articles that also described the potential beneficial effects of the massive circulation of information and knowledge during health emergencies. We summarised the challenges and opportunities associated with infodemics and misinformation. A third author verified the retrieved data, and another author resolved any disagreements between the inter‑reviewers.",
    "Two authors independently appraised the quality of the included articles using the AMSTAR 2 tool, which consists of 16 domains (Shea et al., 2017). Both reviewers conducted the screening and data extraction independently and in a blinded manner to minimize bias. Each categorical domain was rated using an online platform, and an overall assessment of critical and non‑critical domains was obtained. Any inter‑rater discrepancies were initially resolved through discussion, and if consensus could not be reached, a third reviewer was consulted for arbitration.",
    "Data extraction followed a clear and structured process, with records initially identified through database searches and additional sources such as hand‑searching and references. The PRISMA Flow Diagram outlined the screening process, beginning with 495 records identified through database searches, with an additional 127 duplicate records from other sources. After removing duplicates, 495 records were screened for relevance based on title and abstract. Of these, 310 were excluded because 216 were found to be irrelevant to the research topic, 48 were excluded due to language and accessibility barriers, and 46 lacked empirical data. Following this step, 185 reports were sought for retrieval, but 27 could not be  accessed, leaving 158 reports for eligibility assessment.",
    "At the eligibility stage, a detailed evaluation of the 158 reports was conducted, leading to the exclusion of 82 reports. These were removed due to inappropriate study design (Ahmed et al., 2020), lack of an integrated study (Islam et al., 2020), or lack of relevance to the research objectives (Guess et al., 2020). After this rigorous selection process, 76 studies met the inclusion criteria and were incorporated into the final review. The diagram visually represents the systematic approach used in study selection, ensuring transparency and reproducibility in the research process (Figure 3).",
    "A narrative synthesis approach was used to categorize studies into key themes: misinformation spread, impact, and mitigation. Findings were analyzed thematically, integrating qualitative insights and quantitative summaries. Contradictory results were examined for methodological or contextual variations. Intervention effectiveness—fact‑checking, media literacy, and regulations—was compared across studies. Insights were mapped to existing misinformation frameworks, providing a comprehensive understanding of its dynamics and implications."
  ],
  "Conclusion": [
    "The results of this review revealed that the COVID-19 infodemic manifested through various channels, including social media platforms, traditional news outlets, and interpersonal communication (Kisa and Kisa, 2024; Pulido et al., 2020). Social media emerged as a dominant vector for the dissemination of misinformation, with platforms such as Facebook, Twitter, and YouTube playing pivotal roles. For instance, Joseph et al. analyzed millions of posts across platforms and highlighted that misinformation on COVID-19 was shared at a rate comparable to factual information, often reaching large audiences due to algorithmic amplification (Joseph et al., 2022).",
    "Common themes of misinformation included the origins of the virus, prevention and treatment measures, vaccine safety and efficacy, and conspiracy theories. For example, Islam et al. (2020) identified over 2,300 rumours, stigma, and conspiracy theories circulating across 87 countries, with a significant proportion related to unverified treatments such as ingesting disinfectants or using herbal remedies (Islam et al., 2020). This misinformation not only fuelled public confusion but also led to direct harm; a study by Aghababaeian et al. reported over 700 deaths and thousands of hospitalizations in Iran due to methanol poisoning linked to false beliefs about its protective effects against COVID-19 (Aghababaeian et al., 2020).",
    "Empirical studies also highlighted the adverse effects of the infodemic on public health outcomes. Ferreira Caceres et al. found that exposure to COVID-19 misinformation significantly reduced adherence to preventive measures such as mask‑wearing and social distancing (Ferreira Caceres et al., 2022). Health misinformation significantly erodes trust between patients and healthcare professionals, leading to scepticism about medical advice. This distrust negatively impacts patient adherence to treatments and public health measures (Kbaier et al., 2024). Health misinformation significantly undermines public trust in credible health sources due to insufficient health and digital literacy among users, which is exacerbated by socio-economic disparities. A study explored the long-term impact of an Israeli government digital literacy program for disadvantaged populations, as perceived by participants 1 year after course completion. Interviews conducted a year later revealed that participants primarily joined the program out of cognitive interest, particularly to learn internet applications, followed by career aspirations. Reported benefits included increased knowledge, greater confidence in using technology, empowerment, and improved self‑efficacy. However, participants noted that without ongoing practice or instructor support, much of the acquired knowledge diminished over time, affecting the program’s lasting impact (Lev-On et al., 2020). Additionally, cultural contexts influence the reception of misinformation, making certain demographics more vulnerable (Ismail et al., 2022). Similarly, a survey by Pertwee et al. revealed that vaccine hesitancy increased in populations frequently exposed to anti‑vaccine narratives online, with specific claims about microchip implantation and infertility driving mistrust in vaccine campaigns (Pertwee et al., 2022). The infodemic disproportionately affected vulnerable populations. For example, literacy barriers were evident in communities where access to credible sources of information was limited. A study conducted by Gaysynsky et al. demonstrated that individuals with lower health literacy were more likely to believe and share misinformation, exacerbating disparities in health outcomes (Gaysynsky et al., 2024). Vulnerable populations, particularly those in rural areas or low-income settings, were also found to be more susceptible to believing in conspiracy theories due to limited access to verified information sources (Kisa and Kisa, 2024). Quantitative analyses underscored the scale of the problem. Li et al. (2020) found that 25% of COVID-19-related YouTube videos contained misleading information, collectively amassing over 62 million views (Li et al., 2020). A similar study by Gallotti et al. (2020) reported that up to 40% of COVID-19-related tweets contained misinformation, often driven by bots and coordinated campaigns. Specific case studies, such as the “Pandemic” documentary, exemplify how misinformation campaigns gained traction and sowed widespread skepticism regarding public health interventions (Gallotti et al., 2020). Furthermore, Jon Agley and Yunyu Xiao identified a strong correlation between the virality of misinformation and public mistrust in health authorities, further complicating the pandemic response (Agley and Xiao, 2021). Localized examples also illustrate the impact of the infodemic. In India, misinformation about cow urine as a COVID-19 cure gained significant traction, leading to health risks and public confusion. Similarly, in the United States, conspiracy theories about 5G technology causing COVID-19 resulted in vandalism of telecommunications infrastructure, as documented by Ahmed et al. (2020).",
    "Despite widespread misinformation, certain mitigation strategies showed effectiveness. Collaborative efforts between governments and social media companies to flag or remove false information were reported to reduce the virality of some narratives (Nature, 2021). A case study on Facebook’s partnership with fact‑checking organizations demonstrated that labelling posts as misleading reduced their engagement rates by up to 80% (Aïmeur et al., 2023). However, these efforts often lagged behind the rapid spread of misinformation, highlighting the need for proactive measures.",
    "Additionally, campaigns focusing on increasing health literacy emerged as pivotal. Studies by Paul Machete & Marita Turpin revealed that public awareness programs emphasizing critical thinking and source verification significantly reduced the likelihood of individuals sharing false information (Machete and Turpin, 2020). Similarly, tailored interventions targeting specific myths—such as WHO’s “MythBusters” initiative—proved effective in debunking common misconceptions, particularly when culturally contextualized messages were employed. A study by Birunda et al. proposed Automatic COVID-19 misinformation detection (ACOVMD) in Twitter using a self‑trained semi‑supervised hybrid deep learning model. The experimental results show that the proposed model achieves 80.92% accuracy and 98.15% accuracy in the 10 and 80% label‑seen experiments, respectively (Birunda et al., 2024). A study by Lu et al. embraced uncertainty features within the information environment. It introduced a novel Environmental Uncertainty Perception (EUP) framework for detecting misinformation and predicting its spread on social media, which showed that the EUP alone achieved notably good performance, with detection accuracy at 0.753 and prediction accuracy at 0.71. This study makes a significant contribution to the literature by recognizing uncertainty features within information environments as a crucial factor for improving misinformation detection and spread-prediction algorithms during the pandemic (Lu et al., 2024). A study by Zhao et al. proposed a novel health misinformation detection model which incorporated the central-level features (including topic features) and the peripheral-level features (including linguistic features, sentiment features, and user behavioral features). The model correctly detected about 85% of the health misinformation (Zhao et al., 2021).",
    "The COVID-19 infodemic, a term describing the rapid and widespread dissemination of misinformation and disinformation during the pandemic, is a complex issue influenced by societal, technological, and psychological factors (WHO, 2020a). While digital platforms played a pivotal role in amplifying misinformation, the effectiveness of interventions to mitigate its impact remains debatable. A critical analysis of the mechanisms driving misinformation spread, the limitations of current strategies, and areas requiring further research is necessary to formulate a more comprehensive response. The amplification of misinformation on digital platforms can be attributed to algorithmic biases that prioritize engagement over accuracy. Studies by Cinelli et al. (2020) and Vosoughi et al. (2018) suggest that emotionally charged misinformation spreads more rapidly than factual content (Cinelli et al., 2020; Vosoughi et al., 2018). However, these studies primarily focus on Western social media landscapes, raising concerns about their generalizability to regions with different digital ecosystems and media consumption patterns. Additionally, while social media platforms have introduced measures to curb misinformation, such as fact‑checking partnerships, the effectiveness of these interventions is inconsistent.",
    "While AI can rapidly identify disinformation campaigns, its reliance on pattern recognition increases the likelihood of false positives, particularly when distinguishing between satire and harmful misinformation (Pennycook and Rand, 2022). This contrasts with human fact‑checking efforts, which, though slower, provide nuanced contextual understanding. The interplay between these approaches remains a contentious debate, with some scholars arguing for a hybrid model that combines AI efficiency with human oversight to balance speed and accuracy (Zhang et al., 2023). Others highlight the susceptibility of AI systems to adversarial manipulation, where misinformation creators adapt content to evade automated detection, raising concerns about long-term sustainability. Meanwhile, human fact‑checking, despite its strengths in contextual analysis, faces challenges related to scalability and biases introduced by individual or institutional perspectives. The debate between AI‑driven and human‑led approaches underscores the need for a more integrated strategy that considers the strengths and weaknesses of both methodologies. For instance, Pennycook and Rand (2021) found that flagged misinformation was less likely to be shared, yet Guess et al. (2020) demonstrated that such efforts had minimal impact on users entrenched in misinformation echo chambers (Pennycook and Rand, 2021; Guess et al., 2020).",
    "Despite the various strategies employed to combat the COVID-19 infodemic, significant gaps remain in understanding the psychological mechanisms that drive misinformation adoption and resistance to correction. Future research should prioritize comparative studies that examine the effectiveness of interventions across different sociocultural contexts. Additionally, longitudinal studies assessing the durability of fact‑checking, media literacy programs, and regulatory measures would provide deeper insights into sustainable solutions. Regulatory measures targeting digital platforms have also been proposed to curb the infodemic, yet their implementation remains contentious. Germany’s NetzDG law mandates the removal of illegal content within 24 h, a model cited as effective in reducing hate speech. However, concerns about censorship and freedom of expression complicate its adoption on a global scale.",
    "To effectively address the challenges posed by infodemics, future research must focus on a multi-faceted approach that combines technological, behavioral, social, and policy-driven strategies (Rodrigues et al., 2024; WHO, 2020b; Briand et al., 2021). Given the complex nature of misinformation and its far-reaching consequences, particularly in times of crises like the COVID-19 pandemic, it is essential to identify and explore key areas of intervention. The following areas are crucial to the ongoing effort to combat misinformation, each addressing different dimensions of the issue:",
    "Research into algorithm transparency should aim at developing frameworks for ethical algorithm design. This could involve making algorithmic processes more understandable and accessible to the public, ensuring that platforms are held accountable for the content they promote. Platform-driven interventions, such as X’s Community Notes and Facebook’s misinformation labels, aim to curb misinformation through algorithmic adjustments and user-driven corrections (Tan, 2022; Yu et al., 2024; Dujeancourt and Garz, 2023).",
    "Understanding the psychological factors behind the belief and sharing of misinformation is essential for designing interventions that target the root causes of these behaviors. Psychological theories of belief formation, cognitive biases, and social influence can provide crucial insights into why people are so easily influenced by misinformation. For instance, cognitive biases such as confirmation bias, where individuals seek out information that confirms their pre-existing beliefs, play a critical role in the spread of falsehoods. Emotional responses to misinformation, such as fear or anger, also contribute to its virality, as these emotions increase engagement with content (Pennycook and Rand, 2021; Munusamy et al., 2024).",
    "Misinformation is not bound by borders, and its effects are global. The widespread nature of misinformation, especially on platforms like Facebook, Twitter, and YouTube, means that disinformation can easily cross geographical, political, and cultural boundaries. As a result, tackling the infodemic requires global cooperation and coordination among researchers, policymakers, and technology companies. Future research should focus on fostering international partnerships to share data, research findings, and best practices in combating misinformation (Adams et al., 2023; Desai et al., 2022; New WHO Review Finds, 2022).",
    "While global and national interventions are crucial, community-based strategies are also vital for combating misinformation, especially in regions with limited access to digital literacy resources. Misinformation spreads rapidly in  local communities through word-of-mouth, local media, and interpersonal interactions. Therefore, it is essential to evaluate the efficacy of grassroots efforts in building trust and promoting accurate information within communities (Oxford Academic, 2022; Borges Do Nascimento et al., 2022; Stover et al., 2024).",
    "Regulatory and policy measures have been among the most widely discussed approaches to tackling misinformation (Tan, 2022). However, the effectiveness of these measures remains a subject of debate. Some countries have implemented stringent laws aimed at curbing the spread of false information. For example, Singapore’s Protection from Online Falsehoods and Manipulation Act (POFMA) empowers authorities to issue correction orders to platforms and individuals found spreading falsehoods. While such measures have been successful in curbing some forms of misinformation, they have also raised concerns about censorship and the suppression of dissenting voices (Human Rights Watch, 2018b; Protection from Online Falsehoods and Manipulation Act, 2021)."
  ]
}
